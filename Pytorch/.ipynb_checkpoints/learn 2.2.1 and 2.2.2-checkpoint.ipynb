{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建5*3的矩阵，未初始化，仅分配空间\n",
    "x = t.Tensor(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6277, 0.5451, 0.3011],\n",
       "        [0.7497, 0.6440, 0.8294],\n",
       "        [0.0015, 0.2254, 0.8846],\n",
       "        [0.3291, 0.9545, 0.9283],\n",
       "        [0.8947, 0.6735, 0.0959]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用0-1均匀分布，随机初始化二维数组\n",
    "x = t.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.size()) # 查看x 的形状\n",
    "x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6252, 1.2264, 0.8416],\n",
      "        [0.8098, 1.0568, 1.0254],\n",
      "        [0.7846, 0.5171, 1.0329],\n",
      "        [1.2445, 1.6089, 1.1360],\n",
      "        [1.8559, 1.3410, 0.1308]])\n",
      "tensor([[1.6252, 1.2264, 0.8416],\n",
      "        [0.8098, 1.0568, 1.0254],\n",
      "        [0.7846, 0.5171, 1.0329],\n",
      "        [1.2445, 1.6089, 1.1360],\n",
      "        [1.8559, 1.3410, 0.1308]])\n",
      "tensor([[1.6252, 1.2264, 0.8416],\n",
      "        [0.8098, 1.0568, 1.0254],\n",
      "        [0.7846, 0.5171, 1.0329],\n",
      "        [1.2445, 1.6089, 1.1360],\n",
      "        [1.8559, 1.3410, 0.1308]])\n"
     ]
    }
   ],
   "source": [
    "y = t.rand(5, 3)\n",
    "# 加法的三种写法\n",
    "print(x + y)\n",
    "print(t.add(x, y))\n",
    "result = t.Tensor(5, 3)\n",
    "t.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9975, 0.6813, 0.5405],\n",
      "        [0.0601, 0.4128, 0.1960],\n",
      "        [0.7831, 0.2917, 0.1483],\n",
      "        [0.9154, 0.6544, 0.2077],\n",
      "        [0.9613, 0.6675, 0.0349]])\n",
      "tensor([[0.9975, 0.6813, 0.5405],\n",
      "        [0.0601, 0.4128, 0.1960],\n",
      "        [0.7831, 0.2917, 0.1483],\n",
      "        [0.9154, 0.6544, 0.2077],\n",
      "        [0.9613, 0.6675, 0.0349]])\n",
      "tensor([[1.6252, 1.2264, 0.8416],\n",
      "        [0.8098, 1.0568, 1.0254],\n",
      "        [0.7846, 0.5171, 1.0329],\n",
      "        [1.2445, 1.6089, 1.1360],\n",
      "        [1.8559, 1.3410, 0.1308]])\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "y.add(x)\n",
    "print(y)\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 与 Tensor 操作非常相近可以互相转换\n",
    "a = t.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy() # Tensor -> numpy ndarray\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = t.from_numpy(a) # Numpy ndarray -> Tensor\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "b.add_(1)\n",
    "print(a)\n",
    "print(b) # 共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda 冲起来，把Tensor 转换为 GPU 的Tensor\n",
    "if t.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    for i in range(100):\n",
    "        x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5808, 0.2759, 1.1902,  ..., 0.9314, 0.1514, 0.7891],\n",
      "        [1.4628, 0.4667, 0.8837,  ..., 1.2996, 0.6492, 0.7419],\n",
      "        [0.9709, 0.8504, 1.6131,  ..., 0.8998, 0.7174, 1.0065],\n",
      "        ...,\n",
      "        [1.1400, 0.6167, 1.4326,  ..., 0.8442, 0.9254, 1.5875],\n",
      "        [0.6447, 1.0376, 0.5972,  ..., 0.0663, 0.7005, 0.9886],\n",
      "        [1.0324, 1.2996, 0.9249,  ..., 1.4015, 1.1062, 0.9329]],\n",
      "       device='cuda:0')\n",
      "0.22742223739624023\n",
      "tensor([[1.2660, 0.9225, 0.8124,  ..., 1.2673, 0.5347, 1.4447],\n",
      "        [1.8577, 0.9658, 0.8759,  ..., 0.9360, 0.7457, 1.5035],\n",
      "        [0.5502, 0.8400, 1.6228,  ..., 0.6780, 0.5728, 1.1842],\n",
      "        ...,\n",
      "        [0.9185, 0.3635, 1.0821,  ..., 1.5648, 0.7308, 0.6776],\n",
      "        [1.5607, 0.7822, 1.2979,  ..., 0.7699, 1.2422, 0.9965],\n",
      "        [1.1874, 1.6490, 1.3018,  ..., 1.3351, 0.9788, 1.0807]])\n",
      "0.15854692459106445\n"
     ]
    }
   ],
   "source": [
    "import time, os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "begin = time.time()\n",
    "for i in range(1):\n",
    "    x = t.rand(3733, 3733)\n",
    "    y = t.rand(3733, 3733)\n",
    "    z = t.Tensor(3733, 3733)\n",
    "    \n",
    "    x = x.cuda()\n",
    "    y = y.cuda() \n",
    "    z = z.cuda()\n",
    "    t.add(x, y, out=z)\n",
    "end = time.time()\n",
    "print(z)\n",
    "print(end - begin)\n",
    "begin = time.time()\n",
    "for i in range(1):\n",
    "    x = t.rand(3733, 3733)\n",
    "    y = t.rand(3733, 3733)\n",
    "    z = t.Tensor(3733, 3733)\n",
    "    \n",
    "    t.add(x, y, out=z)\n",
    "end = time.time()\n",
    "print(z)\n",
    "print(end - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "# 使用 Tensor 新建一个 Variable\n",
    "x = Variable(t.ones(2, 2), requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SumBackward0 at 0x260c46586a0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = x.sum() = (x[0][0] + x[0][1] + x[1][0] + x[1][1])\n",
    "# 每个梯度值为1\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5403, 0.5403, 0.5403, 0.5403, 0.5403],\n",
      "        [0.5403, 0.5403, 0.5403, 0.5403, 0.5403],\n",
      "        [0.5403, 0.5403, 0.5403, 0.5403, 0.5403],\n",
      "        [0.5403, 0.5403, 0.5403, 0.5403, 0.5403]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5403, 0.5403, 0.5403, 0.5403, 0.5403],\n",
       "        [0.5403, 0.5403, 0.5403, 0.5403, 0.5403],\n",
       "        [0.5403, 0.5403, 0.5403, 0.5403, 0.5403],\n",
       "        [0.5403, 0.5403, 0.5403, 0.5403, 0.5403]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable 与 Tensor 有近乎一致的接口，在实际使用中可以无缝切换。\n",
    "x = Variable(t.ones(4, 5))\n",
    "y = t.cos(x)\n",
    "x_tensor_cos = t.cos(x.data)\n",
    "print(y)\n",
    "x_tensor_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
